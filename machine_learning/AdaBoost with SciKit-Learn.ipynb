{
 "metadata": {
  "name": "",
  "signature": "sha256:c49d6cc359e29782bf0d36b4fdb037e3725372d7541c1924c953042ce1c3eedd"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we understand the theory, let's try using [scikit-learn](http://scikit-learn.org/stable/index.html) to automate much of the work.\n",
      "\n",
      "First, let's load our data..."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import csv\n",
      "\n",
      "def load_data(filename):\n",
      "    with open(filename) as f:\n",
      "        return np.array([ [ int(x) for x in row] for row in csv.reader(f) ])\n",
      "\n",
      "train_data = load_data(\"adult_data_processed.csv\")\n",
      "classes = train_data[:, -1]\n",
      "train_data = train_data[:, :-1]\n",
      "\n",
      "test_data = load_data(\"adult_test_processed.csv\")\n",
      "test_classes = test_data[:, -1]\n",
      "test_data = test_data[:, :-1]\n",
      "\n",
      "train_data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 1,
       "text": [
        "array([[39,  0,  0, ...,  0,  0,  0],\n",
        "       [50,  0,  0, ...,  0,  0,  0],\n",
        "       [38,  0,  0, ...,  0,  0,  0],\n",
        "       ..., \n",
        "       [58,  0,  0, ...,  0,  0,  0],\n",
        "       [22,  0,  0, ...,  0,  0,  0],\n",
        "       [52,  0,  0, ...,  0,  0,  0]])"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Out of the box Decision Trees"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import tree"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf = tree.DecisionTreeClassifier() #max_depth=1)\n",
      "clf = clf.fit(train_data, classes)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.sum(clf.predict(test_data) == test_classes) / len(test_data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "0.99996928841251809"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So, using nothing at all, we can extremely quickly get nearly perfect classification of the test data using the training data to train!  Sigh...  It's worth noting that the resulting tree is extremely deep and complicated, but seemingly hasn't overfit the training data at the expense of saying something about the test data.\n",
      "\n",
      "If we build smaller trees, then the performance is no better, or worse, that AdaBoost with decision stumps:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf = tree.DecisionTreeClassifier(max_depth=5)\n",
      "clf = clf.fit(train_data, classes)\n",
      "np.sum(clf.predict(test_data) == test_classes) / len(test_data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "0.85209299468689537"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Simulate decision stumps"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If we force the tree to be of depth one, we can \"simulate\" a decision stump.  I say \"simulate\" as the algorithm used, CART, does _not_ find the absolutely \"best\" decision stump.  It is however extremely quick compared to my Python or Numpy versions..."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf = tree.DecisionTreeClassifier(max_depth=1)\n",
      "clf = clf.fit(train_data, classes)\n",
      "np.sum(clf.predict(test_data) == test_classes) / len(test_data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "0.75919044255397561"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So we see that this is actually worse performance than I found with an \"optimal\" decision stump.\n",
      "\n",
      "Let's see what's going on under the hood."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.externals.six import StringIO\n",
      "with open(\"iris.dot\", 'w') as f:\n",
      "    f = tree.export_graphviz(clf, out_file=f)\n",
      "with open(\"iris.dot\") as f:\n",
      "    for line in f:\n",
      "        print(line, end=\"\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "digraph Tree {\n",
        "0 [label=\"X[33] <= 0.5000\\ngini = 0.365640628977\\nsamples = 32561\", shape=\"box\"] ;\n",
        "1 [label=\"gini = 0.1221\\nsamples = 17585\\nvalue = [ 16436.   1149.]\", shape=\"box\"] ;\n",
        "0 -> 1 ;\n",
        "2 [label=\"gini = 0.4943\\nsamples = 14976\\nvalue = [ 8284.  6692.]\", shape=\"box\"] ;\n",
        "0 -> 2 ;\n",
        "}"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can understand this as follows: Box 0 slips the training data on column 33 to be <=0.5.  This partitions the data into boxes 1 and 2, where \"values\" lists the proportion of the split data in class -1 or 1, as the following shows..."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "left = train_data[:, 33] <= 0.5\n",
      "right = train_data[:, 33] > 0.5\n",
      "print( [ np.sum(classes[left] == cl) for cl in [-1, 1] ] )\n",
      "print( [ np.sum(classes[right] == cl) for cl in [-1, 1] ] )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[16436, 1149]\n",
        "[8284, 6692]\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "With AdaBoost"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "With [scikit-learn AdaBoost](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html) this is almost boring easy.  We use the Decision Tree as above, but with a max depth of 1.  The end result is actually a bit better than we got with AdaBoost'ed decision stumps."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.ensemble import AdaBoostClassifier"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "bdt = AdaBoostClassifier(tree.DecisionTreeClassifier(max_depth=1))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "bdt = bdt.fit(train_data, classes)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.sum(bdt.predict(test_data) == test_classes) / len(test_data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 13,
       "text": [
        "0.86124504775651856"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "bdt = AdaBoostClassifier(tree.DecisionTreeClassifier(max_depth=5))\n",
      "bdt = bdt.fit(train_data, classes)\n",
      "np.sum(bdt.predict(test_data) == test_classes) / len(test_data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 14,
       "text": [
        "0.89892816559687971"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Titanic Dataset"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with open(\"titanic_processed.csv\") as f:\n",
      "    data = []\n",
      "    for row in csv.reader(f):\n",
      "        data.append( [ float(row[0]) ] + [ int(x) for x in row[1:]] )\n",
      "print(data[0])\n",
      "\n",
      "test = data[len(data)//2:]\n",
      "train = data[:len(data)//2]\n",
      "\n",
      "classes = [ row[-1] for row in train ]\n",
      "train = [ row[:-1] for row in train ]\n",
      "test_classes = [ row[-1] for row in test ]\n",
      "test = [ row[:-1] for row in test ]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[29.0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1]\n"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Out the box decision tree"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf = tree.DecisionTreeClassifier() #max_depth=1)\n",
      "clf = clf.fit(train, classes)\n",
      "np.sum(clf.predict(test) == test_classes) / len(test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 24,
       "text": [
        "0.81126331811263319"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "With AdaBoost and Decision stumps"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "bdt = AdaBoostClassifier(tree.DecisionTreeClassifier(max_depth=1))\n",
      "bdt = bdt.fit(train, classes)\n",
      "np.sum(bdt.predict(test) == test_classes) / len(test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 30,
       "text": [
        "0.73059360730593603"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "bdt = AdaBoostClassifier(tree.DecisionTreeClassifier(max_depth=5))\n",
      "bdt = bdt.fit(train, classes)\n",
      "np.sum(bdt.predict(test) == test_classes) / len(test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 35,
       "text": [
        "0.81126331811263319"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}