{
 "metadata": {
  "name": "",
  "signature": "sha256:247d93298c90004066d20ef7c1b73661fd3812bc882944354a516078ca3145f5"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Some nice example data sources can be found at:\n",
      "\n",
      "   - [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.html)\n",
      "   - [Kaggle](https://www.kaggle.com/)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Adult Data Set"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "From the [UCI Repository](https://archive.ics.uci.edu/ml/datasets/Adult): \"Predict whether income exceeds $50K/yr based on census data. Also known as \"Census Income\" dataset.\""
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import csv\n",
      "def load_our_csv(filename):\n",
      "    with open(filename) as f:\n",
      "        adult_reader = csv.reader(f)\n",
      "        adult = list(adult_reader)\n",
      "    # Strip zero rows at end\n",
      "    while len(adult[-1]) == 0:\n",
      "        adult.pop()\n",
      "    # Strip white space\n",
      "    adult = [ [x.rstrip().strip() for x in row] for row in adult]\n",
      "    return adult\n",
      "\n",
      "adult = load_our_csv(\"adult.data.csv\")    \n",
      "print(adult[0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['39', 'State-gov', '77516', 'Bachelors', '13', 'Never-married', 'Adm-clerical', 'Not-in-family', 'White', 'Male', '2174', '0', '40', 'United-States', '<=50K']\n"
       ]
      }
     ],
     "prompt_number": 85
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "From reading the data information, the final value is the classification: \"<=50K\" or \">50K\".  The other values are:\n",
      "\n",
      "        age: continuous.\n",
      "        workclass: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked.\n",
      "        fnlwgt: continuous.\n",
      "        education: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.\n",
      "        education-num: continuous.\n",
      "        marital-status: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse.\n",
      "        occupation: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.\n",
      "        relationship: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried.\n",
      "        race: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black.\n",
      "        sex: Female, Male.\n",
      "        capital-gain: continuous.\n",
      "        capital-loss: continuous.\n",
      "        hours-per-week: continuous.\n",
      "        native-country: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There are a couple of approaches to dealing with discrete data:\n",
      "\n",
      "   - Apply some domain knowledge.  Don't have any in this instance, so...\n",
      "   - Convert to a class number 0, 1, ..., N-1\n",
      "   - Expand to N fields where each field is \"yes/no\" answer to \"is this equal to possibility i\"?\n",
      "   \n",
      "So in the \"sex\" case, we'd either encode:\n",
      "\n",
      "   - Female: 0;  Male: 1\n",
      "   - Female: 1,0;  Male: 0,1\n",
      "   \n",
      "The second option, while requiring more storage, makes asking \"or\" questions trivial: \"native-counter == Cambodia or England\" translates to \"feature X==1 or feature Y==1\" while in the 1st encoding we need to ask \"X==1 or X==2\" which e.g. [decision trees](https://en.wikipedia.org/wiki/Decision_tree_learning) aren't usually setup to deal with so well.\n",
      "\n",
      "So, let's keep our data \"binary\": the only question we can ask of a feature is that \"X<=t\" or \"X>t\"."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def is_numeric(column):\n",
      "    for x in column:\n",
      "        try:\n",
      "            int(x)\n",
      "        except ValueError:\n",
      "            return False\n",
      "    return True\n",
      "\n",
      "def process_final_column(entry):\n",
      "    \"\"\"Assume the class is the final entry in each row.\"\"\"\n",
      "    parsedict = {\"<=50K\":-1, \">50K\":1, \"<=50K.\":-1, \">50K.\":1}\n",
      "    return parsedict[entry]\n",
      "\n",
      "def to_binary(entry, choices):\n",
      "    return [ (0 if entry != choice else 1) for choice in choices]\n",
      "\n",
      "if len( set( len(row) for row in adult ) ) > 1:\n",
      "    raise Exception(\"Rows have different lengths!\")\n",
      "num_columns = len(adult[0])\n",
      "size_continuous_columns = dict()\n",
      "choices_for_columns = dict()\n",
      "for column_number in range(num_columns):\n",
      "    column = [ row[column_number] for row in adult ]\n",
      "    if is_numeric(column):\n",
      "        size_continuous_columns[column_number] = len(set(column))\n",
      "    else:\n",
      "        choices_for_columns[column_number] = list(set(column))\n",
      "        if len(set(column)) <= 1:\n",
      "            raise Exception(\"Column {} seems to be constant!\".format(column_number))\n",
      "print(\"Choices in continuous data:\", list(size_continuous_columns.values()) )\n",
      "if min(size_continuous_columns.values()) <= 6:\n",
      "    raise Exception(\"There is a 'continuous' column which looks discrete!\")\n",
      "for col in choices_for_columns:\n",
      "    print(\"Column {} -->\".format(col), choices_for_columns[col])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Choices in continuous data: [73, 21648, 16, 119, 92, 94]\n",
        "Column 1 --> ['Self-emp-not-inc', 'State-gov', 'Federal-gov', 'Local-gov', '?', 'Self-emp-inc', 'Private', 'Without-pay', 'Never-worked']\n",
        "Column 3 --> ['Prof-school', 'Preschool', 'Assoc-acdm', '9th', '10th', '5th-6th', 'Assoc-voc', '7th-8th', 'Doctorate', 'Bachelors', 'HS-grad', '12th', 'Masters', 'Some-college', '1st-4th', '11th']\n",
        "Column 5 --> ['Married-spouse-absent', 'Married-AF-spouse', 'Widowed', 'Never-married', 'Separated', 'Married-civ-spouse', 'Divorced']\n",
        "Column 6 --> ['Craft-repair', 'Machine-op-inspct', 'Handlers-cleaners', 'Prof-specialty', 'Tech-support', 'Priv-house-serv', '?', 'Adm-clerical', 'Sales', 'Protective-serv', 'Other-service', 'Transport-moving', 'Farming-fishing', 'Exec-managerial', 'Armed-Forces']\n",
        "Column 7 --> ['Unmarried', 'Other-relative', 'Not-in-family', 'Husband', 'Wife', 'Own-child']\n",
        "Column 8 --> ['Amer-Indian-Eskimo', 'Black', 'Asian-Pac-Islander', 'Other', 'White']\n",
        "Column 9 --> ['Male', 'Female']\n",
        "Column 13 --> ['England', 'China', 'Japan', 'Outlying-US(Guam-USVI-etc)', 'Greece', 'Columbia', 'Portugal', 'Canada', 'Guatemala', 'Yugoslavia', 'Honduras', 'Haiti', 'Cambodia', 'Philippines', 'Nicaragua', 'Vietnam', 'Iran', 'France', 'Ireland', 'Holand-Netherlands', 'Laos', '?', 'Poland', 'Taiwan', 'Dominican-Republic', 'Puerto-Rico', 'Mexico', 'Peru', 'Trinadad&Tobago', 'United-States', 'Germany', 'India', 'Jamaica', 'Cuba', 'Thailand', 'South', 'Italy', 'Hungary', 'Ecuador', 'Hong', 'El-Salvador', 'Scotland']\n",
        "Column 14 --> ['<=50K', '>50K']\n"
       ]
      }
     ],
     "prompt_number": 86
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We see that sometimes the data also contains a \"?\"..."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "adult_processed = []\n",
      "for row in adult:\n",
      "    newrow = []\n",
      "    for cn, entry in enumerate(row[:-1]):\n",
      "        if cn in size_continuous_columns:\n",
      "            newrow.append( int(entry) )\n",
      "        else:\n",
      "            newrow.extend( to_binary(entry, choices_for_columns[cn]) )\n",
      "    newrow.append( process_final_column(row[-1]) )\n",
      "    adult_processed.append( newrow )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 87
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(adult_processed[0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[39, 0, 1, 0, 0, 0, 0, 0, 0, 0, 77516, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 13, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 2174, 0, 40, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1]\n"
       ]
      }
     ],
     "prompt_number": 88
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def save_our_csv(filename, output):\n",
      "    with open(filename, \"w\", newline=\"\") as f:\n",
      "        adult_writer = csv.writer(f)\n",
      "        for row in output:\n",
      "            adult_writer.writerow(row)\n",
      "        \n",
      "save_our_csv(\"adult_data_processed.csv\", adult_processed)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 89
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We'll now do the same for the \"test\" data, but I'll use the same \"choices\", to ensure we end up with the same number of output features (and as a sanity check that the two data sources are similar)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with open(\"adult.test.csv\") as f:\n",
      "    adult_reader = csv.reader(f)\n",
      "    print(\"First line:\", next(adult_reader))\n",
      "    adult_test = list(adult_reader)\n",
      "# Strip zero rows at end\n",
      "while len(adult_test[-1]) == 0:\n",
      "    adult_test.pop()\n",
      "adult_test = [ [x.rstrip().strip() for x in row] for row in adult_test]\n",
      "    \n",
      "adult_test_processed = []\n",
      "for row in adult:\n",
      "    newrow = []\n",
      "    for cn, entry in enumerate(row[:-1]):\n",
      "        if cn in size_continuous_columns:\n",
      "            newrow.append( int(entry) )\n",
      "        else:\n",
      "            newrow.extend( to_binary(entry, choices_for_columns[cn]) )\n",
      "    newrow.append( process_final_column(row[-1]) )\n",
      "    adult_test_processed.append( newrow )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "First line: ['|1x3 Cross validator']\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "save_our_csv(\"adult_test_processed.csv\", adult_test_processed)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Titanic DataSet"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This is the classic survivor dataset from the Titanic disaster.  Motivated by Kaggle, but they have split the data up and you have to use their system for the \"test\" version.  This is the full version from http://lib.stat.cmu.edu/S/Harrell/data/descriptions/titanic.html"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "titanic_raw = load_our_csv(\"titanic.csv\")    \n",
      "print(titanic_raw[0])\n",
      "print(titanic_raw[1])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['row.names', 'pclass', 'survived', 'name', 'age', 'embarked', 'home.dest', 'room', 'ticket', 'boat', 'sex']\n",
        "['1', '1st', '1', 'Allen, Miss Elisabeth Walton', '29.0000', 'Southampton', 'St Louis, MO', 'B-5', '24160 L221', '2', 'female']\n"
       ]
      }
     ],
     "prompt_number": 98
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So we don't need the first row, and columns 0 (PassengerId), 3 (Name) aren't useful.  To make compatible with our other data, we'll move the 1st remaining column to the end, and change to +/- 1."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "titanic = titanic_raw[1:]\n",
      "survived_dict = { \"0\": \"-1\", \"1\" : \"1\"}\n",
      "titanic = [ row[1:3] + row[4:] for row in titanic ]\n",
      "titanic = [ [row[0]] + row[2:] + [survived_dict[row[1]]] for row in titanic ]\n",
      "print(\"Length of data:\", len(titanic))\n",
      "titanic[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Length of data: 1313\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 104,
       "text": [
        "['1st',\n",
        " '29.0000',\n",
        " 'Southampton',\n",
        " 'St Louis, MO',\n",
        " 'B-5',\n",
        " '24160 L221',\n",
        " '2',\n",
        " 'female',\n",
        " '1']"
       ]
      }
     ],
     "prompt_number": 104
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We'll manually explore and process the data.\n",
      "\n",
      "   - Columns 0, 2, 3, 4, 5, 6, 7 are categorical\n",
      "   - Column 1 is floats with some \"NA\""
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for col in [0,2,3,4,5,6,7]:\n",
      "    print(col, \"------>\", set(row[col] for row in titanic))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0 ------> {'1st', '3rd', '2nd'}\n",
        "2 ------> {'Southampton', '', 'Queenstown', 'Cherbourg'}\n",
        "3 ------> {'', 'Manchester, England', 'Southsea, Hants', 'Janjgir, India / Pennsylvania', 'Lake Arthur, Chavez County, NM', 'Montevideo, Uruguay', 'Paris / Haiti', 'Sweden  Worcester, MA', 'Madrid, Spain', 'Seattle, WA / Toledo, OH', 'Wimbledon Park, London / Hayling Island, Hants', 'Trenton, NJ', 'North Evington, England', 'Isleworth, England', 'Jacksonville, FL', 'Waukegan, Chicago, IL', 'England / San Francisco, CA', 'West Hampstead, London / Neepawa, MB', 'Bulgaria Coon Rapids, IA', 'Catford, Kent / Detroit, MI', 'Aughnacliff, Co Longford, Ireland New York, NY', 'Cornwall / Spokane, WA', 'Liverpool, England Bedford, OH', 'Lexington, MA', 'Stockholm, Sweden', 'Cornwall / Houghton, MI', 'Bennington, VT', 'Syria', 'Halesworth, England', 'Southampton', 'Barcelona, Spain / Havana, Cuba', 'Austria-Hungary', 'Bayside, Queens, NY', 'Glen Ridge, NJ', 'Haverford, PA', 'Amenia, ND', 'Antwerp, Belgium / Stanton, OH', 'Isle of Wight, England', 'Co Longford, Ireland New York, NY', 'New York, NY / Briarcliff Manor NY', 'Myren, Sweden New York, NY', 'Sweden Akeley, MN', 'Guernsey / Montclair, NJ and/or Toledo, Ohio', 'Guntur, India / Benton Harbour, MI', 'Syria Ottawa, ON', 'Denmark Hill, Surrey / Chicago', 'Broomfield, Chelmsford, England', 'Bangkok, Thailand / Roseville, IL', 'Bronx, NY', 'Ilford, Essex / Winnipeg, MB', 'Upper Burma, India Pittsburgh, PA', 'England / Bennington, VT', 'Milwaukee, WI', 'London / Staten Island, NY', 'Finland / Washington, DC', 'Effington Rut, SD', 'East Providence, RI', 'Bournmouth, England', 'Hartford, CT', 'Gunnislake, England / Butte, MT', 'Cape Town, South Africa / Seattle, WA', 'Ilfracombe, Devon', 'West Hoboken, NJ', 'Bath, England / Massachusetts', 'Oskarshamn, Sweden Minneapolis, MN', 'Calgary, AB', 'Boston, MA', 'England / Hartford, CT', 'Aberdeen / Portland, OR', 'Co Athlone, Ireland New York, NY', 'London /  East Orange, NJ', 'India / Pittsburgh, PA', 'Fond du Lac, WI', 'Belfast, NI', 'Somerset / Bernardsville, NJ', 'Hong Kong New York, NY', 'Ascot, Berkshire / Rochester, NY', 'Deer Lodge, MT', 'Goteborg, Sweden Huntley, IL', 'Spain / Havana, Cuba', 'Tampico, MT', 'Streatham, Surrey', 'Bristol, Avon / Jacksonville, FL', 'Bournemouth, England', 'London Brooklyn, NY', 'Glasgow', 'Syria Kent, ON', 'Springfield, MA', 'Greenwich, CT', 'Duluth, MN', 'St Ives, Cornwall / Hancock, MI', 'Asarum, Sweden Brooklyn, NY', 'Worcester, England', 'Elkins Park, PA', 'England / Detroit, MI', 'Plymouth, Dorset / Houghton, MI', 'Washington, DC', 'Tofta, Sweden Joliet, IL', 'Wiltshire, England Niagara Falls, NY', 'Brooklyn, NY', 'Bishopstoke, Hants / Fayette Valley, ID', 'Russia', 'Austria', 'Sittingbourne, England / San Diego, CA', 'Hornsey, England', 'Sydney, Australia', 'Provo, UT', 'Krakudden, Sweden Moune, IL', 'Clevedon, England', 'Seattle, WA', 'Stoughton, MA', 'Elizabeth, NJ', 'Illinois, USA', 'Cincinatti, OH', 'Spain', 'Buffalo, NY', 'England Brooklyn, NY', 'Youngstown, OH', 'Berne, Switzerland / Central City, IA', 'Altdorf, Switzerland', 'Green Bay, WI', 'Strood, Kent, England Detroit, MI', 'Russia New York, NY', 'England Salt Lake City, Utah', 'London New York, NY', 'Frankfort, KY', 'Scituate, MA', 'Moscow / Bronx, NY', 'Birkdale, England Cleveland, Ohio', 'Weston-Super-Mare, Somerset', 'Cologne, Germany', 'New Forest, England', 'Ruotsinphyhtaa, Finland New York, NY', 'Guernsey / Elizabeth, NJ', 'Holley, NY', 'Cornwall / Clear Creek, CO', 'Middleburg Heights, OH', 'Sweden Winnipeg, MN', 'New York, NY / Washington, DC', 'Chicago, IL', 'London / Fort Byron, NY', \"St Anne's-on-Sea, Lancashire\", 'Stanton, IA', 'Guernsey, England / Edgewood, RI', 'Nice, France', 'Providence, RI', 'England', 'New York, NY', 'Stockholm, Sweden / Washington, DC', 'Indianapolis, IN', 'England / Sacramento, CA', 'Belmont, MA', 'London / Birmingham', 'St Denys, Southampton, Hants', 'Kilmacowen, Co Sligo, Ireland New York, NY', 'Tranvik, Finland New York', 'Brookline, MA', 'Co Clare, Ireland Washington, DC', 'Harrow-on-the-Hill, Middlesex', 'Buenos Aires, Argentina / New Jersey, NJ', 'New York, NY / Greenwich CT', 'Barre, Co Washington, VT', 'Lucca, Italy / California', 'St Andrews, Guernsey', 'Ireland New York, NY', 'Toronto, ON', 'Harrow, England', 'Goteborg, Sweden / Rockford, IL', 'London, England Norfolk, VA', 'Mt Airy, Philadelphia, PA', 'Cambridge, MA', 'Guernsey / Wilmington, DE', 'Southington / Noank, CT', 'Australia Fingal, ND', 'Co Sligo, Ireland New York, NY', 'London / Middlesex', 'Sweden Worcester, MA', 'Warwick, England', 'Paris / Montreal, PQ', 'Helsinki, Finland Ashtabula, Ohio', 'Tuxedo Park, NY', 'Pomeroy, WA', 'Ireland Brooklyn, NY', 'Paris', 'Taalintehdas, Finland Hoboken, NJ', 'Gallipolis, Ohio / ? Paris / New York', 'Minneapolis, MN', 'Perkins County, SD', 'Victoria, BC', 'India / Rapid City, SD', 'Bergen, Norway', 'Foresvik, Norway Portland, ND', 'Cornwall / Akron, OH', 'West Bromwich, England Pontiac, MI', 'Detroit, MI', 'Co Sligo, Ireland Hartford, CT', 'Sarnia, ON', 'Bristol, England Cleveland, OH', 'Ottawa, ON', 'Roachdale, IN', 'England Albion, NY', 'Medeltorp, Sweden Chicago, IL', 'Brighton, Sussex', 'Norrlot, Sweden Chicago, IL', 'Argentina', 'Krakoryd, Sweden Bloomington, IL', 'Sweden Joliet, IL', 'Salo, Finland Astoria, OR', 'Syria Fredericksburg, VA', 'Portugal / Sau Paulo, Brazil', 'Germantown, Philadelphia, PA', 'Oslo, Norway Cameron, WI', 'Finland Sudbury, ON', 'Ireland Philadelphia, PA', 'Haverford, PA / Cooperstown, NY', 'Co Cork, Ireland Roxbury, MA', 'Cooperstown, NY', 'Treherbert, Cardiff, Wales', 'London / Montreal, PQ', 'Ireland', 'Dorchester, MA', 'Bristol, England / New Britain, CT', 'Dagsas, Sweden Fower, MN', 'Brennes, Norway New York', 'Haddenfield, NJ', 'Croatia', 'Kingwilliamstown, Co Cork, Ireland New York, NY', 'St Ives, Cornwall / Houghton, MI', 'Little Onn Hall, Staffs', 'Bulgaria Chicago, IL', 'Merrill, WI', 'Columbus, OH', 'Denmark / New York, NY', 'Plymouth, England', 'Brockton, MA', 'England Oglesby, IL', 'San Francisco, CA', 'Deephaven, MN / Cedar Rapids, IA', 'Huntington, WV', 'England / Philadelphia, PA', 'Zurich, Switzerland', 'Karberg, Sweden Jerome Junction, AZ', 'Walthamstow, England', 'Cornwall / Hancock, MI', 'Tokyo, Japan', 'St Leonards-on-Sea, England Ohio', 'Oslo, Norway Bayonne, NJ', 'Winnipeg, MB', 'England New York, NY', 'Rotherfield, Sussex, England Essex Co, MA', 'Denver, CO', 'Albany, NY', 'Cornwall', 'Syria Youngstown, OH', 'Montreal, PQ', 'Kingston, Surrey', 'Greece', 'Belgium Detroit, MI', 'Austria Niagara Falls, NY', 'Westcliff-on-Sea, Essex', 'Belgium  Montreal, PQ', 'St Ives, Cornwall / Calumet, MI', 'Cornwall, England Houghton, MI', 'Bridgerule, Devon', 'St Louis, MO', 'Pondersend, England / New Durham, NJ', 'London  Vancouver, BC', 'Ireland Chicago, IL', 'Syria New York, NY', 'Bridgwater, Somerset, England', 'London, England / Marietta, Ohio and Milwaukee, WI', 'Finland / Minneapolis, MN', 'Geneva, Switzerland / Radnor, PA', 'New York, NY /  Stamford CT', 'Brunswick, ME', 'Norway Los Angeles, CA', 'Petworth, Sussex', 'Surbiton Hill, Surrey', 'Portugal', 'Brighton, MA', 'Hessle, Yorks', 'Basel, Switzerland', 'Newark, NJ', 'Swindon, England', 'Paris, France', 'Yoevil, England / Cottage Grove, OR', 'Penzance, Cornwall / Akron, OH', 'Paris, France / New York, NY', 'Liverpool, England / Belfast', 'Greensburg, PA', 'Lima, Peru', 'Lyndhurst, England', 'London, England', 'Kontiolahti, Finland / Detroit, MI', 'Milford, NH', 'Harrisburg, PA', 'Hamilton, ON', 'Rochester, NY', 'Lower Clapton, Middlesex or Erdington, Birmingham', 'Stockholm, Sweden New York', 'Ballydehob, Co Cork, Ireland New York, NY', 'Devon, England Wichita, KS', 'Liverpool / Montreal, PQ', 'Co Limerick, Ireland Sherbrooke, PQ', 'Vadsbro, Sweden Ministee, MI', 'Paris, France New York, NY', 'Paris /  New York, NY', 'Portland, OR', 'New York, NY / Ithaca, NY', 'London / Paris', 'Southampton / New York, NY', 'London Skanteales, NY', 'Sault St Marie, ON', 'Dowagiac, MI', 'Bromsgrove, England / Montreal, PQ', 'West Kensington, London', 'Union Hill, NJ', 'Auburn, NY', 'Liverpool', 'Los Angeles, CA', 'Worcester, MA', 'Halifax, NS', 'Ruotsinphytaa, Finland New York, NY', 'Skara, Sweden / Rockford, IL', 'West Haven, CT', 'London / Chicago, IL', 'East Orange, NJ', 'St James, Long Island, NY', 'Weston-Super-Mare / Moose Jaw, SK', 'Rotterdam, Netherlands', 'Woodford County, KY', 'Greenport, NY', 'Mexico City, Mexico', 'Norwich / New York, NY', 'London / New York, NY', 'Belfast', 'Omaha, NE', 'Austria-Hungary / Germantown, Philadelphia, PA', 'Hudson, NY', 'Plymouth, Devon / Detroit, MI', 'Sweden Chicago, IL', 'Lakewood, NJ', 'Mamaroneck, NY', 'Dorking, Surrey, England', 'Devonport, England', 'Vancouver, BC', 'Guernsey', 'Sweden / Arlington, NJ', 'Bryn Mawr, PA, USA', 'St Austall, Cornwall', 'Copenhagen, Denmark', 'Philadelphia, PA', 'Elmira, NY / Orange, NJ', 'London / Winnipeg, MB', 'Glasgow / Bangor, ME', 'East Bridgewater, MA', 'London', 'Pennsylvania', 'Cornwall / Camden, NJ', 'Chelsea, London', 'Italy Philadelphia, PA', 'New Britain, CT', 'Co Cork, Ireland Charlestown, MA', 'Bryn Mawr, PA', 'Winnipeg, MN', 'Montreal, PQ / Chesterville, ON', 'Windsor, England New York, NY', 'Kingwilliamstown, Co Cork, Ireland Glens Falls, NY', 'Folkstone, Kent / New York, NY', 'Bournemouth, England Newark, NJ', '?Havana, Cuba', 'Hartford, Huntingdonshire'}\n",
        "4 ------> {'', 'C-148', 'C-62', 'B-3', 'E-12', 'B-86', 'C-89', 'B-18', 'D-35', 'C-125', 'D-?', 'D-38', 'B-?', 'C-51', 'B-35', '2131', 'A-11', 'C-99', 'C-93', 'C-49', 'D-7', 'C-7', 'C-22', 'C-55 (?C-95)', 'C-85', 'A-23', 'C-104', 'B-51/3/5', 'B-5', 'B-58/60', 'C-103', 'C-126', 'F-?', 'E-101', 'B-102', 'D-21/2', 'A-36', 'D-56', 'C-83', 'B-49', 'C-45', 'C-97', 'B-82/4', 'E77', 'E-?', 'C26', 'C22', 'C-6', 'C-91', 'C-101', 'C-87', 'A-31', 'B-52/4/6', 'F-33'}\n",
        "5 ------> {'', '28220 L32 10s', '248749 L13', '17754', '17485', '229236 L13', '248744 L13', '230080 L26', '250647', '392091', '17608', '7076', '17754 L224 10s 6d', '113780 L28 10s', '34218 L10 10s', '27849', '17483 L221 15s 7d', '17604 L39 12s', '112058 Complimentary', '24160 L221', '112059', '230136 L39', '17593 L56 18s 7d', '13502 L77', '17613 L27 14s 5d', '17608 L262 7s 6d', '36973 L83 9s 6d', '17483', '17593', '17485 L56 18s 7d', '11755 L39 12s', '17477 L69 6s', '112058', '111361 L57 19s 7d', '13529 L26 5s', '17582 L153 9s 3d', '248698 L13', '17755 L512 6s', 'L15 1s', '17610 L27 15s 5d', '17591 L50 9s 11d'}\n",
        "6 ------> {'', '12', '(96)', '(17)', '(241)', '(15)', '4', '(286)', '(81)', '(295)', '5/7', '(108)', '(122)', '(101)', '(189)', '(79)', 'B', '(43)', '(258)', '14/12', 'A', '(287)', '(121)', '(148)', '15', '(293)', '(165)', '9', '(46)', '(62)', '(245)', '(169)', '(230)', '(18)', '(140)', '7', '(97)', '(283)', '(292)', '(72)', '(38)', '(209)', '(305)', '(126)', '10', '(135)', '(175)', '(236)', '2', '6', '(271)', '13', '(89)', '(307)', '14/D', '(299)', '8', '(52)', '(142)', '(35)', '(19)', '(80)', '(174)', '(22)', '(269)', '(256)', '(124)', '(234)', '(294)', '(263)', '(110)', '(207)', '(275)', '3', '(260)', '16', '(147)', '(171)', '(133)', '(45)', '(75)', '(149)', '5', '(232)', '(172)', '(208)', 'C', 'D', '11', '(190)', '(249)', '(130)', '(322)', '(166)', '(103)', '(259)', '(297)', '1', '(109)', '14'}\n",
        "7 ------> {'male', 'female'}\n"
       ]
      }
     ],
     "prompt_number": 111
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So columns 3 (home.dest), 4 (room), 5 (ticket), 6 (boat) are problematic as they contain many categories.\n",
      "\n",
      "   - If you correlate \"boat\" with \"survived\" you'll see that those who didn't survive either don't have a boat, or the boat is in brackets.\n",
      "   - So column 6 can be ignored.\n",
      "   - If you list the names of the people who share the same \"home.dest\" or \"room\" or \"ticket\" then you'll see these are basically families.  So these carry some information.\n",
      "   - What I'm going to do is to ignore column 3, and for columns 4 and 5 replace them with a count of how many people share the same room or ticket."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tickets = set( row[5] for row in titanic )\n",
      "for tk in tickets:\n",
      "    if tk==\"\": continue\n",
      "    print(tk, [ rrow[3] for rrow, row in zip(titanic_raw[1:], titanic) if row[5] == tk ])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "28220 L32 10s ['Drew, Mr James Vivian', 'Drew, Mrs James Vivian (Lulu Thorne Christian)', 'Drew, Master Marshall Brines']\n",
        "248749 L13 ['Harbeck, Mr William H.']\n",
        "17754 ['Bidois, Miss Rosalie', 'Robbins, Mr Victor']\n",
        "17485 ['Francatelli, Ms Laura Mabel']\n",
        "229236 L13 ['Fox, Mr Stanley H.']\n",
        "248744 L13 ['Aldworth, Mr Charles Augustus']\n",
        "230080 L26 ['Navratil, Master Edmond Roger', 'Navratil, Mr Michel', 'Navratil, Master Michel M.']\n",
        "250647 ['Ponesell, Mr Martin']\n",
        "392091 ['Aks, Mrs Sam (Leah Rosen)', 'Aks, Master Philip']\n",
        "17608 ['Chaudanson, Ms \\tVictorine']\n",
        "7076 ['Adahl, Mr Mauritz Nils Martin']\n",
        "17754 L224 10s 6d ['Astor, Colonel John Jacob', 'Astor, Mrs John Jacob (Madeleine Talmadge Force)', 'Endres, Miss Caroline Louise']\n",
        "113780 L28 10s ['Gracie, Colonel Archibald IV']\n",
        "34218 L10 10s ['Troutt, Miss Edwina Celia']\n",
        "27849 ['Buss, Miss Kate']\n",
        "17483 L221 15s 7d ['Straus, Mr Isidor', 'Straus, Mrs Isidor (Ida Blun)']\n",
        "17604 L39 12s ['Rheims, Mr George Lucien']\n",
        "112058 Complimentary ['Ismay, Mr Joseph Bruce']\n",
        "24160 L221 ['Allen, Miss Elisabeth Walton', 'Madill, Miss Georgette Alexandra', 'Robert, Mrs Edward Scott (Elisabeth Walton McMillan)']\n",
        "112059 ['Harrison, Mr William \\tHenry']\n",
        "230136 L39 ['Becker, Mrs Allen Oliver (Nellie E. Baumgardner)', 'Becker, Miss Marion Louise', 'Becker, Master Richard F.', 'Becker, Miss Ruth Elizabeth']\n",
        "17593 L56 18s 7d ['Guggenheim, Mr Benjamin']\n",
        "13502 L77 ['Andrews, Miss Kornelia Theodosia', 'Hogeboom, Mrs John C. (Anna Andrews)', 'Longley, Miss Gretchen Fiske']\n",
        "17613 L27 14s 5d ['Rosenbaum (Russell), Miss Edith Louise']\n",
        "17608 L262 7s 6d ['Ryerson, Mr Arthur Larned', 'Ryerson, Mrs Arthur Larned (Emily Maria Borie)', 'Ryerson, Miss Emily Borie', 'Ryerson, Master John Borie', 'Ryerson, Miss Susan (Suzette) Parker']\n",
        "36973 L83 9s 6d ['Harris, Mr Henry Birkhardt', 'Harris, Mrs Henry Birkhardt (Irene Wallach)']\n",
        "17483 ['Bird, Ms Ellen', 'Farthing, Mr John']\n",
        "17593 ['Giglio, Mr Victor']\n",
        "17485 L56 18s 7d ['Duff Gordon, Lady (Lucille Wallace Sutherland)']\n",
        "11755 L39 12s ['Duff Gordon, Sir Cosmo Edmund']\n",
        "17477 L69 6s ['Aubert, Mrs Leontine Pauline']\n",
        "112058 ['Fry, Mr Richard']\n",
        "111361 L57 19s 7d ['Hippach, Miss Jean Gertrude', 'Hippach, Mrs Louis Albert (Ida Sophia Fischer)']\n",
        "13529 L26 5s ['Hart, Mr Benjamin', 'Hart, Mrs Benjamin (Esther)', 'Hart, Miss Eva Miriam']\n",
        "17582 L153 9s 3d ['Graham, Miss Margaret Edith', 'Graham, Mrs William Thompson (Edith Junkins)', 'Shutes, Miss Elizabeth W.']\n",
        "248698 L13 ['Beesley, Mr Lawrence']\n",
        "17755 L512 6s ['Cardeza, Mrs James Warburton Martinez (Charlotte Wardle Drake)', 'Cardeza, Mr Thomas Drake Martinez']\n",
        "L15 1s ['Pernot, Mr Rene']\n",
        "17610 L27 15s 5d ['Brown, Mrs James Joseph (Margaret Molly\" Tobin)\"']\n",
        "17591 L50 9s 11d ['Brandeis, Mr Emil']\n"
       ]
      }
     ],
     "prompt_number": 117
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In summary:\n",
      "\n",
      "   - Column 0 is categorical\n",
      "   - Column 1 is floats or \"NA\".  Convert these to \"-1\" and add new binary column indicating we've done this.\n",
      "   - Column 2 is categorical\n",
      "   - Column 3: ignore\n",
      "   - Column 4: Convert to a count of how many passengers share this label\n",
      "   - Column 5: Ditto\n",
      "   - Column 6: ignore\n",
      "   - Column 7: Categorical\n",
      "   - Column 8: The class\n",
      "   \n",
      "Once we've processed columns 4 and 5 they basically look categorical, so convert to categorical."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def count_same(label, column):\n",
      "    return len( [row for row in titanic if row[column] == label] )\n",
      "\n",
      "categorical_columns = [0,2,7]\n",
      "categorical = { col : list(set(row[col] for row in titanic)) for col in categorical_columns }\n",
      "\n",
      "# Pass one\n",
      "titanic_temp = []\n",
      "for row in titanic:\n",
      "    newrow = []\n",
      "    # Float column 1\n",
      "    if row[1] == \"NA\":\n",
      "        newrow.extend( [\"-1\", \"1\"] )\n",
      "    else:\n",
      "        newrow.extend( [row[1], \"0\"] )\n",
      "    for col in categorical_columns:\n",
      "        newrow.extend( to_binary(row[col], categorical[col]) )\n",
      "    # Columns 4, 5\n",
      "    newrow.append(count_same(row[4], 4))\n",
      "    newrow.append(count_same(row[5], 5))\n",
      "    # Class column\n",
      "    newrow.append( row[-1] )\n",
      "    titanic_temp.append(newrow)\n",
      "    \n",
      "# Pass two\n",
      "categorical[-2] = list(set(row[-2] for row in titanic_temp))\n",
      "categorical[-3] = list(set(row[-3] for row in titanic_temp))\n",
      "titanic_processed = []\n",
      "for row in titanic_temp:\n",
      "    newrow = row[0:-3]\n",
      "    newrow.extend( to_binary(row[-3], categorical[-3]) )\n",
      "    newrow.extend( to_binary(row[-2], categorical[-2]) )\n",
      "    newrow.append(row[-1])\n",
      "    titanic_processed.append(newrow)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 127
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(titanic_processed[0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['29.0000', '0', 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, '1']\n"
       ]
      }
     ],
     "prompt_number": 128
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "save_our_csv(\"titanic_processed.csv\", titanic_processed)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 129
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}