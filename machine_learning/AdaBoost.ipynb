{
 "metadata": {
  "name": "",
  "signature": "sha256:35ad50520fc54d21b4c073a8e9ecb8d272cd25f9b21b104e2ef6f90186680f4d"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The following is motivated by Jemery Kun's post: [Weak Learning and Boosting](http://jeremykun.com/2015/05/18/boosting-census/).  Let's explore this a little bit, and compare to [Wikipedia](https://en.wikipedia.org/wiki/AdaBoost).\n",
      "\n",
      "The **Setup** is that we have a \"classification\" problem: a set $X$ of possibilities, and we wish to assign each $x\\in X$ to one of two classes, $+1, -1$.  We also have a family of \"weak learners\" $H$ (this is a technical term, see Kun's post, but in practise it works remarkably well to just assume $H$ is a collection of classifiers which aren't terribly good, but can do better than random guessing).\n",
      "\n",
      "Our overall classifier will be a weighted sum of weak learners: $F_T(x) = \\sum_{t=1}^T \\alpha_t h_t(x)$ where $\\alpha_t > 0$ is our weight and $h_t\\in X$.  Then $F_T$ estimates the class of $x$ by looking at the sign, and we can interpret the magnitude as being a measure of \"certainty\".  We'll construct this iteratively: if we already have $F_{T-1}$ and an \"error function\" $Er(f)$ where $f:X\\rightarrow\\mathbb R$ is a \"classifier\" (allowed to be real-valued, with the same interpretation as just given to $F_T$).\n",
      "\n",
      "Let $x_1,\\cdots,x_n$ be the totality of our data, and $(y_i)\\in{\\pm 1}$ be the classes each $x_i$ belongs to.  If we choose $Er(f) = \\sum_i \\exp(-y_i f(x_i))$ then we can analytically derive the AdaBoost algorithm, see [Wikipedia](https://en.wikipedia.org/wiki/AdaBoost#Derivation) or Kun's post again.  In principle, it seems we could use a different error function, but this would lead to a different algorithm.\n",
      "\n",
      "The algorithm is then as follows:\n",
      "\n",
      "   - Choose initial weights $w_{1,i} = 1/n$ for each $i$.\n",
      "   - For each $t=1,\\cdots, T$:\n",
      "      - Find the weak learner $h_t$ which minimises $\\min_{h\\in H} \\sum_{y_i \\not= h(x_i)} w_{t,i}$.\n",
      "      - Let $\\epsilon_t = \\sum_{y_i \\not= h_t(x_i)} w_{t,i} / \\sum_i w_{t,i}$.\n",
      "      - Set $\\alpha_t = \\frac12\\log( (1-\\epsilon_t) / \\epsilon_t )$.\n",
      "      - Set $w_{t+1, i} = w_{t,i} e^{-y_i \\alpha_t h_t(x_i)}$.\n",
      "   - The final classifier is $\\sum_{t=1}^T \\alpha_t h_t$.\n",
      "   \n",
      "(Aside: This is different from the [Discrete AdaBoost Algorithm](https://en.wikipedia.org/wiki/AdaBoost#Example_Algorithm_.28Discrete_AdaBoost.29) seems to be different, and I think contains a typo.)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This algorithm is not what Kun does, and the reason why is the innocuous \"Find the weak learner $h_t$ which minimises...\"  In practise, $H$ might well be effectively infinite (think: a regression model) or even (as we'll use below) if finite, then rather large.  If $n$ is also rather large, then just searching over $H$ could be prohibitively expensive.\n",
      "\n",
      "Instead, suppose we use a [Bootstrapping](https://en.wikipedia.org/wiki/Bootstrapping_%28statistics%29) algorithm: we select samples  $z_1,\\cdots,z_k$ from our population $(x_i)$ (with replacement), but with the probability dictated by $w_{t,i}$.  That is, we treat $(w_{t,i})_i$ as an unnormalised probability distribution. Let $y_i'$ be the class of $z_i$, and then we find the $h\\in H$ which minimises the error $\\frac1k |\\{i : y'_i \\not= h(z_i) \\}|$.\n",
      "\n",
      "Then $(z_i)$ are iid samples from a distribution, say $Z$.  For any function $g:X\\rightarrow\\mathbb R$ we have that $\\mathbb E(g(Z)) = \\sum_i \\mathbb P(Z=x_i) g(x_i) = K^{-1}\\sum_i w_{t,i} g(x_i)$ where $K=\\sum_i w_{t,i}$.  If we set $g(x) = 1_{h(x) \\not= y}$ then\n",
      "$\\frac1k |\\{i : y'_i \\not= h(z_i) \\}|$ is the sample mean and so an estimator of $\\mathbb E(g(Z)) = K^{-1} \\sum_{h(x_i)\\not=y_i} w_{t,i}$.\n",
      "\n",
      "Thus finding the $h$ which minimises the error rate on the sampled data $(z_i)$ does approxate the \"exact\" minimisation."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Decision stumps"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A [Decision Stump](https://en.wikipedia.org/wiki/Decision_stump) is a single level decision tree.  Given a real-valued feature $x$ we simply classify via\n",
      "\n",
      "   - $x\\leq x_0$ then choose class $-1$\n",
      "   - otherwise choose class $1$\n",
      "   \n",
      "This looks asymmetric, but we can cheat and simply also consider the feature $-x$.  Notice that this scheme also copes with \"binary\" data, $x$ being $0,1$ valued.\n",
      "\n",
      "I don't see any efficient way to find $x_0$ however."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class DecisionStump:\n",
      "    def __init__(self, data, realclasses):\n",
      "        \"\"\"data is a list of values, and realclasses is a list of +/-1 indicating which class example i comes from.\n",
      "        Initialises self.x0 so that the classifier given by i -> -1 iff x[i] <= self.x0 obtains the highest\n",
      "        rate of correct classification.  self.correct is set to count of correctly classified data.\"\"\"\n",
      "        choices = set(data)\n",
      "        choices.add( min(choices) - 1)\n",
      "        self.x0 = min(choices, key = lambda choice : self.number_correct(data, realclasses, choice) )\n",
      "        self.correct = len(data) - self.number_correct(data, realclasses, self.x0)\n",
      "\n",
      "    def number_correct(self, data, realclasses, x0):\n",
      "        return ( sum( cl != -1 for x, cl in zip(data, realclasses) if x <= x0)\n",
      "                      + sum( cl != 1 for x, cl in zip(data, realclasses) if x > x0) )\n",
      "\n",
      "    def classify(self, entry):\n",
      "        return -1 if entry <= self.x0 else 1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "d = DecisionStump([1,2,3,4], [-1,1,1,-1])\n",
      "d.x0, d.correct"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "(1, 3)"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Our example data will be a list of rows, where each row is a list of features, ending with the class +/-1."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import itertools\n",
      "\n",
      "class DecisionStumpFromData:\n",
      "    def __init__(self, data):\n",
      "        \"\"\"data is a list of rows, each row being a list of features ending with the class +/-1.\"\"\"\n",
      "        \"\"\"Sets self.x0, self.column, self.negated=True/False.\"\"\"\n",
      "        num_columns = len(data[0]) - 1\n",
      "        choices = itertools.product(range(num_columns), [True, False])\n",
      "        classes = [row[-1] for row in data]\n",
      "        results = []\n",
      "        for (col, negated) in choices:\n",
      "            column = [ row[col] for row in data ]\n",
      "            if negated:\n",
      "                column = [-x for x in column]\n",
      "            d = DecisionStump(column, classes)\n",
      "            results.append( (col, negated, d.x0, d.correct) )\n",
      "        self.column, self.negated, self.x0, _ = max(results, key = lambda tup : tup[3])\n",
      "        \n",
      "    def classify(self, row):\n",
      "        entry = row[self.column]\n",
      "        if self.negated:\n",
      "            entry = -entry\n",
      "        return -1 if entry <= self.x0 else 1\n",
      "    \n",
      "    def __repr__(self):\n",
      "        return \"DecisionStumpFromData(x0={}, column={}, negated={})\".format(self.x0, self.column, self.negated)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import csv\n",
      "with open(\"adult_data_processed.csv\") as f:\n",
      "    data = [ [ int(x) for x in row] for row in csv.reader(f) ]\n",
      "print(data[0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[39, 0, 1, 0, 0, 0, 0, 0, 0, 0, 77516, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 13, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 2174, 0, 40, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1]\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import random\n",
      "d = DecisionStumpFromData(random.sample(data, 500))\n",
      "print(sum( d.classify(row) == row[-1] for row in data ) / len(data))\n",
      "d"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.8009274899419551\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "DecisionStumpFromData(x0=6849, column=63, negated=False)"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The final general code we need is to sample from data according to some (unnormalised) density."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import random\n",
      "\n",
      "class WeightedReSampler:\n",
      "    def __init__(self, data, weights):\n",
      "        \"\"\"data is a list, and weights should be a list of non-negative but unnormalised density.\"\"\"\n",
      "        self.data = list(zip(data, weights))\n",
      "        self.data.sort(key = lambda pair : pair[1])\n",
      "        self.normalisation = sum(p for _,p in self.data)\n",
      "        \n",
      "    def sample(self):\n",
      "        prob = random.random() * self.normalisation\n",
      "        cumulative = 0\n",
      "        for x, p in self.data:\n",
      "            cumulative += p\n",
      "            if cumulative >= prob:\n",
      "                return x\n",
      "        return self.data[-1][0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import collections\n",
      "values = [1,2,3,4,5]\n",
      "prob = [1,10,2,5,13]\n",
      "w = WeightedReSampler(values, prob)\n",
      "c = collections.Counter(w.sample() for _ in range(100000))\n",
      "for x, p in zip(values,prob):\n",
      "    print(x,c[x] / 100000, p/sum(prob))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1 0.03224 0.03225806451612903\n",
        "2 0.3206 0.3225806451612903\n",
        "3 0.0653 0.06451612903225806\n",
        "4 0.16086 0.16129032258064516\n",
        "5 0.421 0.41935483870967744\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "AdaBoost code"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import math\n",
      "\n",
      "def WLDecisionStump(samples):\n",
      "    \"\"\"Input: samples is list of input rows in our format.\n",
      "    Output should be an object which supports a `classify` method.\"\"\"\n",
      "    return DecisionStumpFromData(samples)\n",
      "\n",
      "class AdaBoost:\n",
      "    def __init__(self, weaklearner, data, classes):\n",
      "        \"\"\"weaklearner should be a function `DecisionStumpFromData(samples)` which returns an object\n",
      "        representing a learner and which has a method `classify` compatible with the data.\n",
      "        `data` should be a list of data, and `classes` should be a list of the class each row is in.\"\"\"\n",
      "        self.data = data\n",
      "        self.classes = classes\n",
      "        self.weaklearner = weaklearner\n",
      "        self.weights = [1.0] * len(data)\n",
      "        self.weightsum = []\n",
      "        \n",
      "    def add(self, alpha, h):\n",
      "        self.weightsum.append( (alpha, h) )\n",
      "        \n",
      "    def classify(self, row):\n",
      "        x = sum( alpha * h.classify(row) for alpha, h in self.weightsum )\n",
      "        return -1 if x <=0 else 1\n",
      "    \n",
      "    def fraction_correct(self):\n",
      "        return sum( cl == self.classify(row) for cl, row in zip(self.classes, self.data) ) / len(self.data)\n",
      "    \n",
      "    def step(self):\n",
      "        wrs = WeightedReSampler(self.data, self.weights)\n",
      "        samples = [wrs.sample() for _ in range(500)] # 500 = Magic Number\n",
      "        ht = self.weaklearner(samples)\n",
      "        et = sum( w for w, row, cl in zip(self.weights, self.data, self.classes) if  cl != ht.classify(row) )\n",
      "        et /= sum(self.weights)\n",
      "        alphat = math.log( (1-et) / et ) / 2\n",
      "        self.add(alphat, ht)\n",
      "        self.weights = [ w * math.exp(-cl * ht.classify(row) * alphat) for row, cl, w in zip(self.data, self.classes, self.weights) ]\n",
      "        return et"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ab = AdaBoost(WLDecisionStump, data, [row[-1] for row in data])\n",
      "for _ in range(20):\n",
      "    print(\"Error rate for current distribtuion:\", ab.step())\n",
      "    print(\"Currently being classified correctly:\", ab.fraction_correct())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Error rate for current distribtuion: 0.19934891434538252\n",
        "Currently being classified correctly:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.8006510856546175\n",
        "Error rate for current distribtuion:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.23442856790808406\n",
        "Currently being classified correctly:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.8006510856546175\n",
        "Error rate for current distribtuion:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.30887346639994756\n",
        "Currently being classified correctly:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.8375664138079297\n",
        "Error rate for current distribtuion:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.398865099327166\n",
        "Currently being classified correctly:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.8375664138079297\n",
        "Error rate for current distribtuion:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.37922841386576533\n",
        "Currently being classified correctly:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.8398697828690765\n",
        "Error rate for current distribtuion:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.4576967617618337\n",
        "Currently being classified correctly:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.8409446884309449\n",
        "Error rate for current distribtuion:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.43245453935562705\n",
        "Currently being classified correctly:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.8417124781179939\n",
        "Error rate for current distribtuion:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.44350422410057744\n",
        "Currently being classified correctly:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.8417431897054759\n",
        "Error rate for current distribtuion:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.48181796748004646\n",
        "Currently being classified correctly:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.8417739012929578\n",
        "Error rate for current distribtuion:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.42284154386104605\n",
        "Currently being classified correctly:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.8438622892417309\n",
        "Error rate for current distribtuion:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.44396546792338687\n",
        "Currently being classified correctly:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.8431559227296459\n",
        "Error rate for current distribtuion:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.47482393995812533\n",
        "Currently being classified correctly:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.8434937501919474\n",
        "Error rate for current distribtuion:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.45288463470554713\n",
        "Currently being classified correctly:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.8465341973526611\n",
        "Error rate for current distribtuion:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.4810402728303837\n",
        "Currently being classified correctly:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.8464420625902153\n",
        "Error rate for current distribtuion:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.48896085799099315\n",
        "Currently being classified correctly:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.8466263321151071\n",
        "Error rate for current distribtuion:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.4673913992404276\n",
        "Currently being classified correctly:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.8450907527410092\n",
        "Error rate for current distribtuion:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.4575815795023859\n",
        "Currently being classified correctly:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.8465341973526611\n",
        "Error rate for current distribtuion:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.4932799883375123\n",
        "Currently being classified correctly:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.8471484291023003\n",
        "Error rate for current distribtuion:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.4775008452778082\n",
        "Currently being classified correctly:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.8474248333896379\n",
        "Error rate for current distribtuion:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.5043126789414861\n",
        "Currently being classified correctly:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.84730198703971\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with open(\"adult_test_processed.csv\") as f:\n",
      "    test_data = [ [ int(x) for x in row] for row in csv.reader(f) ]\n",
      "sum( row[-1] == ab.classify(row) for row in test_data ) / len(test_data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 11,
       "text": [
        "0.8407911304935353"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So, we get 85% accuracy via a \"blind\" method.  It's worth noting that the \"best\" single Decision stump already does nearly as well.  I think this is the \"capital-gain\" field."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "d = DecisionStumpFromData(random.sample(data, 500))\n",
      "print(sum( d.classify(row) == row[-1] for row in test_data ) / len(test_data))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.8025859156659808\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "With Titanic Dataset"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with open(\"titanic_processed.csv\") as f:\n",
      "    data = []\n",
      "    for row in csv.reader(f):\n",
      "        data.append( [ float(row[0]) ] + [ int(x) for x in row[1:]] )\n",
      "print(data[0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[29.0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1]\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Let's split it 50/50 to training and test\n",
      "test = data[len(data)//2:]\n",
      "train = data[:len(data)//2]\n",
      "data = None"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ab = AdaBoost(WLDecisionStump, train, [row[-1] for row in train])\n",
      "for _ in range(20):\n",
      "    print(\"Error rate for current distribtuion:\", ab.step())\n",
      "    print(\"Currently being classified correctly:\", ab.fraction_correct())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Error rate for current distribtuion: 0.18292682926829268\n",
        "Currently being classified correctly: 0.8170731707317073\n",
        "Error rate for current distribtuion:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.3695895522388071\n",
        "Currently being classified correctly: 0.8170731707317073\n",
        "Error rate for current distribtuion:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.3856162995034663\n",
        "Currently being classified correctly: 0.8170731707317073\n",
        "Error rate for current distribtuion:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.4536299104376144\n",
        "Currently being classified correctly: 0.8170731707317073\n",
        "Error rate for current distribtuion:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.4718019337613785\n",
        "Currently being classified correctly: 0.8170731707317073\n",
        "Error rate for current distribtuion:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.4655980856804382\n",
        "Currently being classified correctly: 0.8170731707317073\n",
        "Error rate for current distribtuion:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.42840797645324163\n",
        "Currently being classified correctly: 0.8170731707317073\n",
        "Error rate for current distribtuion:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.448109071687742\n",
        "Currently being classified correctly: 0.8170731707317073\n",
        "Error rate for current distribtuion:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.47828009985885445\n",
        "Currently being classified correctly: 0.8170731707317073\n",
        "Error rate for current distribtuion:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.46450473295955974\n",
        "Currently being classified correctly: 0.823170731707317\n",
        "Error rate for current distribtuion:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.4673892044515985\n",
        "Currently being classified correctly: 0.823170731707317\n",
        "Error rate for current distribtuion:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.4448613594721155\n",
        "Currently being classified correctly: 0.8185975609756098\n",
        "Error rate for current distribtuion:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.46423790562935907\n",
        "Currently being classified correctly: 0.8170731707317073\n",
        "Error rate for current distribtuion:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.45370045784918\n",
        "Currently being classified correctly: 0.8201219512195121\n",
        "Error rate for current distribtuion:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.4725106728482527\n",
        "Currently being classified correctly: 0.823170731707317\n",
        "Error rate for current distribtuion:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.46013352230626164\n",
        "Currently being classified correctly: 0.823170731707317\n",
        "Error rate for current distribtuion:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.45389205190235105\n",
        "Currently being classified correctly: 0.823170731707317\n",
        "Error rate for current distribtuion:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.4620365821459514\n",
        "Currently being classified correctly: 0.8262195121951219\n",
        "Error rate for current distribtuion:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.46977061635295114\n",
        "Currently being classified correctly: 0.823170731707317\n",
        "Error rate for current distribtuion:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.4746185942190889\n",
        "Currently being classified correctly: 0.8262195121951219\n"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sum( row[-1] == ab.classify(row) for row in test ) / len(test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 20,
       "text": [
        "0.7290715372907154"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}